{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abf7c49d",
   "metadata": {},
   "source": [
    "딥러닝 코드를 작성하다 보면, 신경망 모델 자체를 코딩하는 시간보다 그 모델을 훈련시키는 코드를 짜는데 더 오랜 시간이 걸리기 마련입니다. 데이터 입력을 준비하는 부분도 이에 해당합니다.\n",
    "\n",
    "$토치텍스트^{torchText}$는 자연어 처리 문제 또는 텍스트에 관한 머신러닝이나 딥러닝을 수행하는 데이터를 읽고 전처리하는 코드를 모아둔 라이브러리입니다. 텍스트 분류나 언어 모델, 그리고 기계 번역의 경우에도 토치텍스트를 활용하여 쉽게 텍스트 파일을 읽어내 훈련에 사용합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f901802d",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30bbd8b",
   "metadata": {},
   "source": [
    "### 예제 코드"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a750facf",
   "metadata": {},
   "source": [
    "자연어 처리 분야에서 주로 쓰이는 학습 데이터는 크게 3가지 형태로 분류할 수 있습니다.\n",
    "\n",
    "<br></br>\n",
    "![](./images/4-9-2-inputoutput.jpg)\n",
    "<br></br>\n",
    "\n",
    "따라서 우리는 이 3가지 종류의 데이터 형태를 다루는 방법을 실습합니다. 토치텍스트는 훨씬 복잡하고 정교한 함수들을 제공합니다. 여기서는 제공되는 함수들의 사용을 최소화하여 복잡하지 않고 간단한 방식으로 구현해보고자 합니다.\n",
    "\n",
    "보통 *Field*라는 클래스를 통해 우리가 읽고자 하는 텍스트 파일 내의 필드를 먼저 정의합니다. 텍스트 파일 내에서 탭을 사용하여 필드를 구분하는 방식을 자연어 처리 분야의 입력에서 가장 많이 사용합니다. 쉼표의 경우에는 텍스트 내부에 많이 포함될 수 있으므로, 쉼표를 구분 문자로 사용하는 것은 위험한 선택이 될 가능성이 높습니다. 이렇게 정의된 각 필드를 Dataset 클래스를 통해 읽어들입니다.\n",
    "\n",
    "읽어들인 코퍼스는 미리 주어진 미니배치 크기에 따라서 나뉠 수 있도록 이터레이터에 들어갑니다. 미니배치를 구성하는 과정에서 미니배치 내에서 문장의 길이가 다를 경우에는 필요에 따라 문장의 앞 또는 뒤에 $패딩^{padding}(PAD)$을 삽입합니다. 이 패딩은 추후 소개할 BOS, EOS와 함께 하나의 단어 또는 토큰과 같은 취급을 받습니다. 이후에 훈련 코퍼스에 대해 어휘 사전을 만들어 각 토큰을 숫자로 맵핑하는 작업을 수행하면 됩니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48acd7b1",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4118fe98",
   "metadata": {},
   "source": [
    "#### 코퍼스와 레이블 읽기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8678db",
   "metadata": {},
   "source": [
    "첫번째 예제는 한줄에서 클래스와 텍스트가 탭으로 구분된 데이터의 입력을 받는 내용입니다. 이런 예제는 주로 **텍스트 분류**에서 사용됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b186324",
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://stackoverflow.com/a/66517960\n",
    "from torchtext.legacy import data\n",
    "\n",
    "class DataLoader(object):\n",
    "    \n",
    "    def __init__(self, train_fn, valid_fn,\n",
    "                 batch_size = 64,\n",
    "                 device = 1,\n",
    "                 max_vocab = 999999,\n",
    "                 min_freq = 1,\n",
    "                 use_eos = False,\n",
    "                 shuffle = True):\n",
    "        \n",
    "        super(DataLoader, self).__init__()\n",
    "        \n",
    "        ## Define field of the input file\n",
    "        ## The input file consists of two fields.\n",
    "        self.label = data.Field(sequential = False,\n",
    "                                use_vocab = True,\n",
    "                                unk_token = None)\n",
    "        self.text = data.Field(use_vocab = True,\n",
    "                               batch_first = True,\n",
    "                               include_lengths = False,\n",
    "                               eos_token = \"<EOS>\" if use_eos else None)\n",
    "        \n",
    "        ## Those defined two columns will be delimited by TAB\n",
    "        ## Thus, we use TabularDataset to load two columns in the input file.\n",
    "        ## We would have two separate input files: train_fn, valid_fn\n",
    "        ## Files consist of two columns: label field and text field.\n",
    "        \n",
    "        train, valid = data.TabularDataset.splits(path = \"\",\n",
    "                                                  train = train_fn,\n",
    "                                                  validation = valid_fn,\n",
    "                                                  format = \"tsv\",\n",
    "                                                  fields = [('label', self.label),\n",
    "                                                            ('text', self.text)])\n",
    "        \n",
    "        ## Those loaded dataset would be feeded into each iterator:\n",
    "        ## train iterator and valid iterator\n",
    "        ## We sort input sentences by length, to group similar lengths\n",
    "        self.train_iter, self.valid_iter = data.BucketIterator.splits((train, valid),\n",
    "                                                                      batch_size = batch_size,\n",
    "                                                                      device = \"cuda:%d\" % device if device >=0 else \"cpu\",\n",
    "                                                                      shuffle = shuffle,\n",
    "                                                                      sort_key = lambda x: len(x.text),\n",
    "                                                                      sort_within_batch = True)\n",
    "        \n",
    "        ## At last, we make a vocabulary for label and text field.\n",
    "        ## It is making mapping table between words and indice.\n",
    "        self.label.build_vocab(train)\n",
    "        self.text.build_vocab(train, max_size = max_vocab, min_freq = min_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18f24de",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433fb03e",
   "metadata": {},
   "source": [
    "#### 코퍼스 읽기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10856e6d",
   "metadata": {},
   "source": [
    "이 예제는 한 라인이 텍스트로만 채워져 있을때를 위한 코드입니다. 주로 **언어 모델**을 훈련시키는 상황에서 쓸 수 있습니다. **LanguageModelDataset**을 통해 미리 정의된 필드를 텍스트 파일에서 읽어들입니다. 이때 각 문장의 길이에 따라 정렬을 통해 비슷한 길이의 문장끼리 미니배치를 만들어줍니다. 이 작업을 통해서 매우 상이한 길이의 문장들이 하나의 미니배치에 묶여 훈련 시간에서 손해보는 것을 방지합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "780dda7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.legacy import data, datasets\n",
    "\n",
    "PAD, BOS, EOS = 1, 2, 3\n",
    "\n",
    "class DataLoader():\n",
    "    \n",
    "    def __init__(self,\n",
    "                 train_fn,\n",
    "                 valid_fn,\n",
    "                 batch_size = 64,\n",
    "                 device = \"cpu\",\n",
    "                 max_vocab = 99999999,\n",
    "                 max_length = 255,\n",
    "                 fix_length = None,\n",
    "                 use_bos = True,\n",
    "                 use_eos = True,\n",
    "                 shuffle = True):\n",
    "        \n",
    "        super(DataLoader, self).__init__()\n",
    "        \n",
    "        self.text = data.Field(sequential = True,\n",
    "                               use_vocab = True,\n",
    "                               batch_first = True,\n",
    "                               include_lengths = True,\n",
    "                               fix_length = fix_length,\n",
    "                               init_token = \"<BOS>\" if use_bos else None,\n",
    "                               eos_token = \"<EOS>\" if use_eos else None)\n",
    "        \n",
    "        train = LanguageModelDataset(path = train_fn,\n",
    "                                     fields = [(\"text\", self.text)],\n",
    "                                     max_length = max_length)\n",
    "        \n",
    "        valid = LanugageModelDataset(path = valid_fn,\n",
    "                                     fields = [(\"text\", self.text)],\n",
    "                                     max_length = max_length)\n",
    "        \n",
    "        self.train_iter = data.BucketIterator(train,\n",
    "                                              batch_size = batch_size,\n",
    "                                              device = \"cude%d\" % device if device >= 0 else \"cpu\",\n",
    "                                              shuffle = shuffle,\n",
    "                                              sort_key = lambda x: -len(x.text),\n",
    "                                              sort_within_batch = True)\n",
    "        \n",
    "        self.valid_iter = data.BucketIterator(valid,\n",
    "                                              batch_size = batch_size,\n",
    "                                              device = \"cude%d\" % device if device >= 0 else \"cpu\",\n",
    "                                              shuffle = False,\n",
    "                                              sort_key = lambda x: -len(x.text),\n",
    "                                              sort_within_batch = True)       \n",
    "        \n",
    "        self.text.build_vocab(train, max_size = max_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3e059bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanugageModelDataset(data.Dataset):\n",
    "    \n",
    "    def __init__(self, path, fields, max_length = None, **kwargs):\n",
    "        if not isinstance(fields[0], (tuple, list)):\n",
    "            fields = [(\"text\", fields[0])]\n",
    "            \n",
    "        examples = []\n",
    "        with open(path) as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                \n",
    "                if max_length and max_length < len(line.split()):\n",
    "                    continue\n",
    "                \n",
    "                if line != \"\":\n",
    "                    examples.append(data.Example.fromlist([line], fields))\n",
    "                    \n",
    "        super(LanugageModelDataset, self).__init__(examples, fields, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b345b826",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ded5ce",
   "metadata": {},
   "source": [
    "#### 병렬 코퍼스 읽기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be97919b",
   "metadata": {},
   "source": [
    "다음 예제는 **텍스트로만 채워진 2개의 파일을 동시에 입력 데이터**로 읽어들이는 코드입니다. 이때 두 파일의 코퍼스는 **병렬 데이터**로 취급되어 같은 라인끼리 맵핑되어야 하므로, 같은 라인 수로 채워져 있어야 합니다.\n",
    "\n",
    "주로 기계번역이나 요약등에 사용될 수 있습니다. 탭을 사용하여 하나의 파일에서 2개의 *$열^{column}$에 각 언어의 문장을 표현하는 것도 한가지 방법이 될 수 있습니다. 그렇다면 앞서 소개한 `TabularDataset` 클래스를 이용하면 됩니다. 또한 `LanguageModelDataset`과 마찬가지로 길이에 따라서 미니배치를 구성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "949af207",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torchtext.legacy import data, datasets\n",
    "\n",
    "PAD, BOS, EOS = 1, 2, 3\n",
    "\n",
    "class DataLoader():\n",
    "    \n",
    "    def __init__(self,\n",
    "                 train_fn = None,\n",
    "                 valid_fn = None,\n",
    "                 exts = None,\n",
    "                 batch_size = 64,\n",
    "                 device = \"cpu\",\n",
    "                 max_vocab = 99999999,\n",
    "                 max_length = 255,\n",
    "                 fix_length = None,\n",
    "                 use_bos = True,\n",
    "                 use_eos = True,\n",
    "                 shuffle = True,\n",
    "                 dsl = False):\n",
    "        \n",
    "        super(DataLoader, self).__init__()\n",
    "        \n",
    "        self.src = data.Field(sequential = True,\n",
    "                              use_vocab = True,\n",
    "                              batch_first = True,\n",
    "                              include_lengths = True,\n",
    "                              fix_length = fix_length,\n",
    "                              init_token = \"<BOS>\" if dsl else None,\n",
    "                              eos_token = \"<EOS>\" if dsl else None)\n",
    "        \n",
    "        self.tgt = data.Field(sequential = True,\n",
    "                              use_vocab = True,\n",
    "                              batch_first = True,\n",
    "                              include_lengths = True,\n",
    "                              fix_length = fix_length,\n",
    "                              init_token = \"<BOS>\" if dsl else None,\n",
    "                              eos_token = \"<EOS>\" if dsl else None)\n",
    "        \n",
    "        if train_fn is not None and valid_fn is not None and exts is not None:\n",
    "            train = TranslationDataset(path = train_fn,\n",
    "                                       exts = exts,\n",
    "                                       fields = [(\"src\", self.src),\n",
    "                                                 (\"tgt\", self.tgt)],\n",
    "                                       max_length = max_length)\n",
    "            \n",
    "            valid = TranslationDataset(path = valid_fn,\n",
    "                                       exts = exts,\n",
    "                                       fields = [(\"src\", self.src),\n",
    "                                                 (\"tgt\", self.tgt)],\n",
    "                                       max_length = max_length)\n",
    "\n",
    "            self.train_iter = data.BucketIterator(train,\n",
    "                                                  batch_size = batch_size,\n",
    "                                                  device = \"cude%d\" % device if device >= 0 else \"cpu\",\n",
    "                                                  shuffle = shuffle,\n",
    "                                                  sort_key = lambda x: len(x.tgt) + (max_length * len(x.src)),\n",
    "                                                  sort_within_batch = True)\n",
    "            \n",
    "            self.valid_iter = data.BucketIterator(valid,\n",
    "                                                  batch_size = batch_size,\n",
    "                                                  device = \"cude%d\" % device if device >= 0 else \"cpu\",\n",
    "                                                  shuffle = False,\n",
    "                                                  sort_key = lambda x: len(x.tgt) + (max_length * len(x.src)),\n",
    "                                                  sort_within_batch = True)\n",
    "            \n",
    "            self.src.build_vocab(train, max_size = max_vocab)\n",
    "            self.tgt.build_vocab(train, max_size = max_vocab)\n",
    "            \n",
    "        def load_vocab(self, src_vocab, tgt_vocab):\n",
    "            self.src.vocab = src_vocab\n",
    "            self.tgt.vocab = tgt_vocab                                                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7026470",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(data.Dataset):\n",
    "    \n",
    "    @staticmethod\n",
    "    def sort_key(ex):\n",
    "        return data.interleave_keys(len(ex.src), len(ex.trg))\n",
    "    \n",
    "    def __init__(self, path, exts, fields, max_length = None, **kwargs):\n",
    "        if not isinstance(fields[0], (tuple, list)):\n",
    "            fields = [(\"src\", fields[0]), (\"trg\", fields[1])]\n",
    "            \n",
    "        if not path.endswith(\".\"):\n",
    "            path += \".\"\n",
    "            \n",
    "        src_path, trg_path = tuple(os.path.expanduser(path + x) for x in exts)\n",
    "        \n",
    "        examples = []\n",
    "        with open(src_path, encoding = \"utf-8\") as src_file, open(trg_path, encoding = \"utf-8\") as trg_file:\n",
    "            for src_line, trg_line in zip(src_file, trg_file):\n",
    "                src_line, trg_line = src_line.strip(), trg_line.strip()\n",
    "                \n",
    "                if max_length and max_length < max(len(src_line.split()),\n",
    "                                                   len(trg_line.split())):\n",
    "                    continue\n",
    "                    \n",
    "                if src_line != \"\" and trg_line != \"\":\n",
    "                    examples.append(data.Example.fromlist([src_line, trg_line], fields))\n",
    "        \n",
    "        super().__init__(examples, fields, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad2e860",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

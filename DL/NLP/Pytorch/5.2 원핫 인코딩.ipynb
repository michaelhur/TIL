{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27a6bd44",
   "metadata": {},
   "source": [
    "단어는 특정 개념을 표현하며, 그에 따라 어휘 분류 체계를 갖거나 의미간 유사도를 가집니다. 단어와 단어는 서로 매우 비슷한 의미를 지닐 수도 있고, 반대의 의미를 지닐 수도 있으며, 전혀 상관없을 수도 있습니다.\n",
    "\n",
    "하지만 단어는 불연속적인 심볼로, 그 내부의 의미는 유사할 수 있지만 겉 형태는 다른 경우가 많습니다.\n",
    "\n",
    "이런 언어의 특징을 활용하여 머신러닝에 잘 적용해야 합니다. 머신러닝은 보통 데이터 기반으로 모델을 훈련하고, 새로운 데이터가 왔을대 잘 예측할 수 있어야 합니다. 따라서 주어진 훈련 데이터를 잘 활용하여 최대한 많은 정보를 추출해 많은 것을 배울 수 있어야 합니다.\n",
    "\n",
    "* 고양이는 좋은 반려동물입니다.\n",
    "* 강아지는 훌륭한 애완동물입니다.\n",
    "\n",
    "예를 들어 이런 훈련 데이터가 주어졌을때, 우리는 \"고양이\" $\\approx$ \"강아지\", \"좋은\" $\\approx$ \"훌륭한\", \"반려동물\" $\\approx$ \"애완동물\" 이라는 상식을 활용하여 유사한 단어들로부터 부족한 정보를 더 보완하기를 바랄 것입니다.\n",
    "\n",
    "단어를 컴퓨터가 인지할 수 있는 수치로 바꾸는 가장 간단한 방법은 **벡터로 표현하는 것**입니다. 그 중에서도 가장 간단한 방법은 **원한 인코딩**입니다. 말 그대로 단 하나의 1과 나머지 수 많은 0들로 표현된 인코딩 방식입니다. 원한 인코딩 벡터의 차원은 보통 전체 $어휘^{vocabulary}$의 개수가 되며, 보통 그 숫자는 매우 큰 숫자입니다.\n",
    "\n",
    "<br></br>\n",
    "\n",
    "v $\\in \\{0, 1\\}^{\\lvert V \\rvert}$ where v is one-hot vector and |V| is vocabulary size\n",
    "\n",
    "<br></br>\n",
    "\n",
    "전체 단어에 대해 원핫 벡터를 구성한다면 다음과 같은 형태가 됩니다.\n",
    "\n",
    "<br></br>\n",
    "\n",
    "|단어|사전내 순서|원핫 벡터|\n",
    "|---|---------|-------|\n",
    "|강아지|8|0,0,0,0,0,0,0,1,0,0,0,...,0|\n",
    "|개|9|0,0,0,0,0,0,0,0,1,0,...,0|\n",
    "|고양이|10|0,0,0,0,0,0,0,0,0,1,...,0|\n",
    "|...|...|...|\n",
    "\n",
    "<br></br>\n",
    "\n",
    "단어는 불연속적인 심볼로써 **이산 확률 변수**로 나타냅니다. 이산 확률 변수는 그 값을 불연속적으로 가질 수밖에 없습니다. 따라서 원핫 벡터는 이산 확률 분포로부터 뽑아낸 샘플이라고 할 수 있습니다. 주로 멀티눌리 확률 분포가 될 것입니다.\n",
    "\n",
    "<br></br>\n",
    "![](./images/5-2-1-prob.jpg)\n",
    "<br></br>\n",
    "\n",
    "이처럼 $사전^{dictionary}$내의 각 단어를 원핫 인코딩 방식을 통해 벡터로 나타낼 수 있습니다. 그런데 이 표현 방식은 여러 가지 문제점을 가집니다. 먼저 벡터의 차원이 너무 커졌습니다. 각 벡터는 단 하나의 1을 갖고 나머지는 0으로 가득 찼습니다. 이처럼 벡터의 많은 부분이 0으로 채워진 벡터를 **$희소벡터^{sparse vector}$**라고 합니다. 이러한 희속 벡터의 가장 큰 문제점은 벡터 간 연산을 할때 결괏값이 0이 된다는 것입니다.\n",
    "\n",
    "<br></br>\n",
    "$[0,0,...,1,0]x[0,1,0,...,0]^T = 0$\n",
    "<br></br>\n",
    "\n",
    "바꿔 말하면 \"강아지\"와 \"개\"라는 단어는 서로 유사한 것인데 이 둘의 유사도는 0이 나올 것이고, \"강아지\"와 \"컴퓨터\"는 상대적으로 관계가 적지만 마찬가지로 이 둘의 유사도도 0이 될 것입니다.\n",
    "\n",
    "이러한 표현 방식은 불리하게 작용할 수 밖에 없습니다. \"강아지\"에 대한 데이터가 적어 일반화하기 힘들때 \"개\"와 관련된 데이터로부터 도움을 받을 수 없기 때문입니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef725ba0",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c3c430",
   "metadata": {},
   "source": [
    "### 차원의 저주"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c8d3af",
   "metadata": {},
   "source": [
    "또 다른 문제는 이런 희소 벡터는 머신러닝에 매우 큰 장벽으로 작용한다는 점입니다. 예를 들어, 정보를 표현하는 데 훨씬큰 차원이 사용되었다면, 작은 차원으로 같은 정보를 표현한 것에 비해 같은 크기의 공간에 표현되는 정보가 상대적으로 훨씬 적을 것이기 때문입니다. 정보를 표현하는 각 점은 매우 낮은 밀도로 희소 하게 퍼져있을 것입니다.\n",
    "\n",
    "<br></br>\n",
    "![](./images/5-2-1-cod.jpg)\n",
    "<br></br>\n",
    "\n",
    "차원이 늘어날수록 이와 같은 문제가 $지수적^{exponential}$으로 늘어납니다. 이런 문제를 차원의 저주라고 부릅니다. 따라서 차원의 저주로부터 벗어나고자 차원을 축소하여 단어를 표현할 필요성을 느낍니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

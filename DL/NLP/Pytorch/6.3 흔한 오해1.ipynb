{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e03a4ddc",
   "metadata": {},
   "source": [
    "단어를 벡터로 매끄럽게 표현하는 방법을 살펴보고 있습니다. 이어서 skip-gram 또는 GloVe를 사용하여 원핫 인코딩의 희소 벡터를 차원 축소해 저차원의 덴스 벡터로 표현하는 방법을 다룰 것입니다.\n",
    "\n",
    "이렇게 훈련한 단어 임베딩 벡터를 추후 우리가 다룰 *텍스트 분류*, *언어 모델*, *번역*등의 딥러닝 모델 입력으로 사용하리라 생각한다는 점입니다. 이 임베딩 벡터를 *$사전\\ 훈련^{pretraining}$*된 임베딩 벡터라고 부릅니다. word2vec을 통해 얻은 단어 임베딩 벡터가 훌륭하게 단어의 특징을 잘 반영하고 있긴 하지만, 텍스트 분류나 언어 모델, 번역의 문제를 해결하는 최적의 벡터 임베딩이라고는 볼 수 없습니다.\n",
    "\n",
    "다시 말하자면 텍스트 분류 또는 기계 번역을 위한 목적 함수는 분명히 word2vec과 다른 형태로 존재합니다. 따라서 다른 목적 함수를 통해 훈련한 임베딩 벡터는 원래의 목적에 맞지 않을 가능 성이 높습니다.\n",
    "\n",
    "예를 들어 긍정/ 부정 감성 분류를 위한 텍스트 분류 문제의 경우에는 \"행복\"이라는 단어가 매우 중요한 특징이 될 것이고, 이를 표현하기 위한 임베딩 벡터가 존재할 것입니다. 하지만 기계번역의 경우 \"행복\"이라는 단어는 그저 일반적인 단어에 지나지 않습니다. 이 분류 문제를 위한 \"행복\"이라는 단어의 임베딩 벡터 값은 긍정/부정 분류 문제의 값과 당연히 달라질 것 입니다. 따라서 문제의 특징을 고려하지 않은 단어 임베딩 벡터는 그다지 좋은 방법이 될 수 없습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15055d9",
   "metadata": {},
   "source": [
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1dcde1",
   "metadata": {},
   "source": [
    "### word2vec 없이 신경망 훈련시키기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37747b4b",
   "metadata": {},
   "source": [
    "word2vec을 사용하여 단어를 저차원의 임베딩 벡터로 변환하지 않아도, 문제의 특징에 맞는 단어 임베딩 벡터를 구할수 있습니다. 파이토치는 $임베딩\\ 계층^{embedding\\ layer}$를 제공합니다. 이 계층은 다음과 같이 편차^{bias}가 없는 선형 계층과 같은 형태를 지닙니다.\n",
    "\n",
    "<br></br>\n",
    "$$y = emb(x) = Wx, \\\\\n",
    "where\\ W \\in R^{d\\ x\\ |v|}\\ and |V| is\\ size\\ of\\ vocabulary.$$\n",
    "<br></br>\n",
    "\n",
    "쉽게 생각하면 W는 d x |V| 크기의 2차원 행렬입니다. 따라서 입력으로 원핫 벡터가 주어지면, W의 특정 \"행\"만 반환합니다.\n",
    "\n",
    "<br></br>\n",
    "![](./images/6-3-1-embedding.jpg)\n",
    "<br></br>\n",
    "\n",
    "따라서 최종적으로 모델로부터 구한 손실값에 따라 역전파 및 경사하강법을 수행하면, 자동적으로 임베딩 계층의 가중치인 W의 값을 구할 수 있을 것입니다.\n",
    "\n",
    "실제 구현에서는 이렇게 큰 임베딩 계층 가중치와 원핫 인코딩 벡터를 곱하는 것은 매우 비효율적이므로, 단순히 테이블에서 검색하는 작업을 수행합니다. 따라서 단어를 나타낼때 원핫 벡터를 굳이 넘겨줄 필요 없이, 1이 존재하는 단어의 인덱스 정수값만 입력으로 넘겨주면 임베딩 벡터를 얻을 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a2687a",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f46c6e1",
   "metadata": {},
   "source": [
    "### 그래도 word2vec을 적용하는 경우"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eba8e2b",
   "metadata": {},
   "source": [
    "그래도 사전 훈련된 단어 임베딩 벡터 적용을 고려해볼 만한 몇몇 상황이 있습니다. 예를 들어 준비된 코퍼스의 양이 너무 적고, 이때 외부로부터 많은 양의 말뭉치를 통해 미리 훈련한 단어 임베딩 벡터를 구한다는 특수한 경우를 가정해볼 수 있습니다.\n",
    "\n",
    "한편으로는 기본 정석대로 먼저 베이스라인^{baseline} 모델을 만든 후, 성능을 끌어올리기 위한 여러가지 방법을 시도할때 사전 훈련된 단어 임베딩 벡터를 사용해볼 수도 있습니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

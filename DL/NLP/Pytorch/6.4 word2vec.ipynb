{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00e63f73",
   "metadata": {},
   "source": [
    "word2vec은 단어를 임베딩하는 두가지 방법으로 CBOW와 Skip-gram을 제시합니다. 두 방법 모두 함께 등장한 단어가 비슷할 수록 비슷한 벡터 값을 가질 것이라는 공통된 가정을 전제합니다.\n",
    "\n",
    "<br></br>\n",
    "![](./images/6-4-1-word2vec.jpg)\n",
    "<br></br>\n",
    "\n",
    "두 방법 모두 윈도우의 크기가 주어지면, 특정 단어를 기준으로 윈도우 내의 주변 단어들을 사용하여 단어 임베딩을 학습합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86f5d33",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314cde70",
   "metadata": {},
   "source": [
    "### 알고리즘 개요: CBOW와 Skip-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850607ea",
   "metadata": {},
   "source": [
    "**CBOW** (Continuous Bag of Words): 신경망은 주변에 나타나는 단어들을 원핫 인코딩된 벡터로 입력받아 해당 단어를 예측하게 합니다.\n",
    "\n",
    "**Skip-gram**: 대상 단어를 원핫 인코딩된 벡터로 입력받아 주변에 나타나는 단어를 예측하는 네트워크를 구성해 단어 임베딩 벡터를 학습합니다.\n",
    "\n",
    "\n",
    "보통 skip-gram이 cbow보다 성능이 뛰어난 것으로 알려져있으면 더 널리 쓰입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcc9a95",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419664c3",
   "metadata": {},
   "source": [
    "### 상세 훈련 방식"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c2ad2b",
   "metadata": {},
   "source": [
    "Skip-gram 방식을 더 자세하게 살펴보겠습니다. Skip-gram은 MLE를 통해 다음 수식의 argmax내의 수식을 최대로 하는 파라미터 $\\theta$를 찾습니다. 이를 통해 $w_t$가 주어졌을때, 앞뒤 n개의 단어 $(w_{t-n}, \\dots, w_{t+n})$를 예측하도록 훈련됩니다. 이때 윈도우의 크기는 n입니다.\n",
    "\n",
    "\n",
    "<br></br>\n",
    "$$\\hat{\\theta} = argmax_{\\theta} \\sum_{t=1}^{T}(\\sum_{i=1}^{n} log P(w_{t-i}|w_t;\\theta) + \\sum_{i=1}^{n} log P(w_{t+i}|w_t;\\theta)) $$\n",
    "<br></br>\n",
    "\n",
    "skip-gram에서도 원핫 인코딩 벡터를 덴스 벡터인 단어 임베딩 벡터로 변환하는 방식을 사용합니다.\n",
    "\n",
    "<br></br>\n",
    "$$\\hat{y} = argmax_{y \\in Y} softmax(W'Wx)$$\n",
    "<br></br>\n",
    "\n",
    "where $W' \\in R^{|V|xd}, W \\in R^{dx|V|}, x \\in \\{0,1\\}^{|V|}$\n",
    "\n",
    "이 수식을 그림으로 표현하면 다음과 같습니다. 수식에서 볼 수 있듯 매우 간단한 구조입니다. softmax 계층은 출력층입니다.\n",
    "\n",
    "<br></br>\n",
    "![](./images/6-4-2-skipgram.jpg)\n",
    "<br></br>\n",
    "\n",
    "W의 각 행은 skip-gram을 통해 얻은 단어 x에 대한 단어 임베딩 벡터가 됩니다. \n",
    "\n",
    "skip-gram을 통해 얻어진 단어 임베딩 벡터들을 PCA를 활용하여 3차원 공간에 표현하면 다음과 같습니다.\n",
    "\n",
    "<br></br>\n",
    "![](./images/6-4-2-pca.jpg)\n",
    "<br></br>\n",
    "\n",
    "차원 축소된 공간에 단어들이 빽빽하게 분포하는 것을 알 수 있으며, 동시에 비슷한 단어들끼리 가까이 분포되어 있음을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e0da47",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e86e04af",
   "metadata": {},
   "source": [
    "### MAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f82d115",
   "metadata": {},
   "source": [
    "#### 베이즈 정리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9205d0a",
   "metadata": {},
   "source": [
    "우리가 구하려는 데이터 D가 주어졌을때 각 클래스 c의 확률을 나타냅니다. 이는 수식으로 나타내면\n",
    "\n",
    "<br></br>\n",
    "$$P(c|D) = \\frac{P(D|c)P(c)}{P(D)} \\\\\n",
    "= \\frac{P(D|c)P(c)}{\\sum_{i=1}^{|C|} P(D|c_i)P(c_i)}\n",
    "$$ \n",
    "<br></br>\n",
    "\n",
    "이때 수식의 각 부분은 명칭을 가집니다.\n",
    "\n",
    "<br></br>\n",
    "![](./images/8-2-1-map.jpg)\n",
    "<br></br>\n",
    "\n",
    "풀고자하는 문제에서 대부분의 경우 P(D)는 구하기 어려우므로, 보통 다음과 같이 접근하기도 합니다. 이때 우리가 원하는 P(c|D) 자체는 클래스 c에 관한 함수입니다.\n",
    "\n",
    "<br></br>\n",
    "$$P(c|D) \\propto P(D|c)P(c)$$\n",
    "<br></br>\n",
    "\n",
    "앞의 성질을 이용하여 주어진 데이터 D를 만족하며 확률을 최대로 하는 클래스 c를 구할 수 있습니다. 이처럼 사후 확률을 최대화하는 클래스 c를 구하는 것을 **Maximum a posterior (MAP)**라고 합니다.\n",
    "\n",
    "<br></br>\n",
    "$$\\hat{c_{MAP}} = argmax_{c \\in C} P(C = c|D)$$\n",
    "<br></br>\n",
    "\n",
    "D가 주어졌을대 가능한 클래스의 집합 c중 사후 확률을 최대로 하는 클래스 D를 선택하는 것입니다.\n",
    "\n",
    "이와 마찬가지로 D가 나타날 가능도를 최대로 하는 클래스 D를 선택하는 것을 **최대가능도 추정 (MLE)**라고 합니다.\n",
    "\n",
    "<br></br>\n",
    "$$\\hat{c_{MLE}} = argmax_{c \\in C} P(D|C=c)$$\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd120d49",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99567fad",
   "metadata": {},
   "source": [
    "#### MLE vs MAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2a43b2",
   "metadata": {},
   "source": [
    "MAP의 경우에 따라 MLE에 비해 더 정확할 수 있습니다. 사전 확률이 반영되기 때문입니다.\n",
    "\n",
    "예를 들어, 범죄 현장에서 범인의 것으로 의심되는 발자국을 발견하고 신발 크기(x)를 측정한 결과 235로 나타났습니다. 이때 범인의 성별(y)를 예측한다고 하면,\n",
    "\n",
    "<br></br>\n",
    "$$\n",
    "Y = \\{male,\\ female\\} \\\\\n",
    "X = \\{\\dots, 225, 230, 235, 240, \\dots\\}$$\n",
    "<br></br>\n",
    "\n",
    "남자일때 신발크기가 235일 확률 $P(x=235|y=male)$은 여자일때 신발크기가 235일 확률 $P(x=235|y=female)$일 확률보다 낮습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3784bdcd",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b38d747",
   "metadata": {},
   "source": [
    "### 나이브 베이즈"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4efe85d",
   "metadata": {},
   "source": [
    "Naive Bayes는 MAP를 기반으로 동작합니다. 대부분의 경우 사후 확률을 바로 구하기 어렵기 때문에, 가능도와 사전 확률의 곱을 통해 클래스 y를 예측합니다.\n",
    "\n",
    "먼저 우리가 알고자하는 값인 사후 확률은 다음과 같습니다. 이는 n개의 단어 $w_1, w_2, \\dots, w_n$이 주어졌을때, 문장이 c 클래스에 속하는 확률값을 의미합니다.\n",
    "\n",
    "<br></br>\n",
    "$$P(y=c | x=w_1,w_2,\\dots,w_n)$$\n",
    "<br></br>\n",
    "\n",
    "이때 x가 복잡한 특징으로 이루어진 데이터라면 훈련 데이터에서 매우 희소할 것이므로 사후 확률뿐만 아니라 가능도 $P(x=w_1, w_2, \\dots, w_n|y=c)$를 구하기 어려울 것입니다. 보통 확률은 코퍼스의 출현 빈도를 통해 추정할 수 있는데, 특징이 복잡할수록 (문장이 복잡하거나 길이가 길어질 수록) 가능도 또는 사후 확률을 만족하는 경우는 코퍼스에 매우 드물 것이기 때문입니다.\n",
    "\n",
    "이때 나이브 베이즈가 강력한 힘을 발휘합니다. 각 특징이 독립적이라고 가정하는 것입니다. 그럼 각 특징의 결합 확률을 각 독립된 확률의 곱으로 근사할 수 있습니다.\n",
    "\n",
    "<br></br>\n",
    "$$P(y=c | x=w_1,w_2,\\dots,w_n) \\propto P(x=w_1, w_2, \\dots, w_n|y=c) \\\\\n",
    "\\approx P(w_1|c)P(w_2|c) \\dots P(w_n|c)P(c) \\\\\n",
    "= \\prod_{i=1}^{n}P(w_i|c)P(c) \\\\\n",
    "= P(c) \\prod_{i=1}^{n}P(w_i|c)$$\n",
    "<br></br>\n",
    "\n",
    "따라서 우리가 구하고자 하는 MAP을 활용한 클래스는 다음과 같이 사후 확률을 최대화하는 클래스가 되고, 이는 나이브 베이즈의 가정에 따라 각 특징들의 확률의 곱에 사전 확률을 곱한 값을 최대화하는 클래스와 같을 것입니다.\n",
    "\n",
    "<br></br>\n",
    "$$\\hat{c} = argmax_{c \\in C} P(y=c|x=w_1, w_2, \\dots, w_n) \\\\\n",
    "\\approx argmax_{c \\in C} \\prod_{i=1}^{n}P(w_i|c)P(c) $$\n",
    "<br></br>\n",
    "\n",
    "이때 사용되는 사전 확률은 다음과 같이 실제 데이터 (코퍼스)에서 출현한 빈도를 통해 추정할 수 있습니다.\n",
    "\n",
    "<br></br>\n",
    "$$P(y=c) \\approx \\frac{Count(c)}{\\sum_{i=1}^{|C|}Count(c_i)}$$\n",
    "<br></br>\n",
    "\n",
    "특징별 가능도 확률 또한 데이터에서 바로 구할 수 있습니다. 만약 모든 특징의 조합이 데이터에 실제로 나타난 횟수를 통해 확률을 구하려 했다면 희소성 문제때문에 구할 수 없었을 것입니다. 하지만 나이브 베이즈의 가정을 통해 코퍼스에서의 출현 빈도를 쉽게 활용할 수 있습니다.\n",
    "\n",
    "<br></br>\n",
    "$$P(w|c) \\approx \\frac{Count(w,c)}{\\sum_{j=1}^{|V|}Count(w_j,c)}$$\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d13d4e1",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132bc09e",
   "metadata": {},
   "source": [
    "### 예제: 감정 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffec69d8",
   "metadata": {},
   "source": [
    "감정 분석은 가장 많이 활용되는 텍스트 분류 기법입니다. 사영자의 댓글이나 리뷰 등을 긍정 또는 부정으로 분류합니다.\n",
    "\n",
    "<br></br>\n",
    "$$\n",
    "C = \\{pos,neg\\} \\\\\n",
    "D = \\{d_1, d2_, \\dots \\}\n",
    "$$\n",
    "<br></br>\n",
    "\n",
    "이와 같이 긍정과 부정으로 클래스의 집합 C가 구성되어있고, 문서 d로 구성된 데이터 D가 있습니다. 이때 \"I am happy to see this movie!\"라는 문장이 주어진다면, 이 문장이 긍정인지 부정인지 판단해보겠습니다.\n",
    "\n",
    "<br></br>\n",
    "$$\n",
    "P(pos |I,am,happy,to,see,this,movie,!) \\\\\n",
    "= \\frac{P(I, am, happy, to, see, this, movie, !|pos)P(pos)}{P(I, am, happy, to, see, this, movie, !)}\n",
    "\\approx \\frac{P(I|pos)P(am|pos)P(happy|pos) \\dots P(!|pos)P(pos)}{P(I, am, happy, to, see, this, movie, !)}\n",
    "$$\n",
    "<br></br>\n",
    "\n",
    "나이브 베이즈를 활용하여 단어의 조합에 대한 확률을 각각 분해할 수 있습니다. 즉, 각 단어의 출현 확률을 독립적이라고 가정한 후에, 결합 가능도 확률을 모두 각각의 가능도 확률로 분해합니다.\n",
    "\n",
    "<br></br>\n",
    "$$\n",
    "P(happy|pos) \\approx \\frac{Count(happy,pos)}{\\sum_{j=1}^{|V|}Count(w_j,pos)} \\\\\n",
    "P(pos) \\approx \\frac{Count(Pos)}{|D|}\n",
    "$$\n",
    "<br></br>\n",
    "\n",
    "이처럼 단순히 코퍼스에서 각 단어의 클래스당 출현 빈도를 계산하는 것만으로도 간단하게 감성분석을 할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416ee4db",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dddd84b",
   "metadata": {},
   "source": [
    "### add-one Smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1efc3a",
   "metadata": {},
   "source": [
    "여기서 문제가 생깁니다. 만약 훈련 데이터에서 $Count(happy, neg)$가 0이였다면 $P(happy|neg) = 0$이 될 것입니다. 하지만 그저 훈련 데이터에 존재하지 않는 경우라는 이유로 해당 샘플의 실제 출현 확률을 0으로 추정하는 것은 매우 위험한 일입니다.\n",
    "\n",
    "<br></br>\n",
    "$$\n",
    "P(happy|neg) \\approx \\frac{Count(happy,neg)}{\\sum_{j=1}^{|V|}Count(w_j,neg)} \\\\\n",
    "\\text{where}\\ Count(happy,neg) = 0\n",
    "$$\n",
    "<br></br>\n",
    "\n",
    "이때 각 출현 횟수에 1을 더해주면 쉽게 문제를 해결할 수 있습니다. 물론 완벽한 해결법은 아니지만 매우 간단한 방법입니다.\n",
    "\n",
    "<br></br>\n",
    "$$\n",
    "\\tilde{P}(w|c) \\approx \\frac{Count(w,c)+1}{(\\sum_{j=1}^{|V|}Count(w_j,c))+|V|}\n",
    "$$\n",
    "<br></br>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ac3010",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e04b14",
   "metadata": {},
   "source": [
    "###  장점과 한계"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2300453",
   "metadata": {},
   "source": [
    "이와 같이 Naive Bayes를 활용하면 단순히 출현 빈도를 세는 것처럼 쉽고 간단하면서도 강력하게 감성 분석을 구현할 수 있습니다. 하지만 **I am not happy to see this movie!**라는 문장이 주어진다면 어떻게 될까요? **not**이라는 단어 한개만 추가되었지만 문장의 뜻은 정반대가 되었습니다.\n",
    "\n",
    "<br></br>\n",
    "$$\n",
    "P(pos |I,am,happy,to,see,this,movie,!) \\\\\n",
    "P(neg |I,am,happy,to,see,this,movie,!)\n",
    "$$\n",
    "<br></br>\n",
    "\n",
    "\"not\"은 \"happy\"를 수식하므로 두 단어를 독립적으로 보는 것은 옳지않습니다.\n",
    "\n",
    "<br></br>\n",
    "$$\n",
    "P(not, happy) \\neq P(not)P(happy)\n",
    "$$\n",
    "<br></br>\n",
    "\n",
    "문장은 단어들이 순서대로 나타나 의미를 이루는 것인 만큼, 각 단어의 출현 여부도 중요하지만 단어간의 순서로 인해 생기는 관계와 정보도 무시할 수 없습니다. 하지만 나이브 베이즈의 \"각 특징은 서로 독립적이다\"라는 가정은 이런 특징을 단순화하여 접근하므로 한계가 있습니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

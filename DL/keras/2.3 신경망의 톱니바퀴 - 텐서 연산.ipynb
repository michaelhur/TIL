{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c2e979b",
   "metadata": {},
   "source": [
    "심층 신경망이 학습한 모든 변환을 수치 데이터 텐서에 적용하는 **Tensor Operation** (텐서 연산)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49982fc9",
   "metadata": {},
   "source": [
    "첫번째 예제에서 Dense 층을 쌓아서 신경망을 만들었음\n",
    "\n",
    "```python\n",
    "keras.layers.Dense(512, activation = 'relu')\n",
    "```\n",
    "\n",
    "이 층은 2D Tensor를 입력받고 새로운 표현인 또 다른 2D Tensor를 반환하는 함수로 해석할 수 있음.\n",
    "\n",
    "구체적으로 보면:\n",
    "\n",
    "```python\n",
    "output = relut(dot(W, input) + b)\n",
    "```\n",
    "\n",
    "3가지 텐서 연산이 있음\n",
    "* 입력 텐서와 텐서 `W`사이의 dot product\n",
    "* dot product의 결과인 2D 텐서와 b 사의 덧셈\n",
    "* 마지막으로 `relu` 연산\n",
    "\n",
    "**relu(x) = max(x, 0)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3497e10",
   "metadata": {},
   "source": [
    "### 2.3.1 원소별 연산"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbbe37d",
   "metadata": {},
   "source": [
    "* `relu` 함수와 덧셈은 원소별 연산 (**element-wise operation**)\n",
    "* 텐서에 있는 각 원소에 독립적으로 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b48b6f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_relu(x):\n",
    "    ## x is 2D numpy array\n",
    "    assert len(x.shape) == 2 \n",
    "    \n",
    "    ## 입력 텐서 자체를 바꾸지 않도록 복사합니다.\n",
    "    x = x.copy()\n",
    "    \n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            x[i,j] = max(x[i,j], 0)\n",
    "            \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01596764",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_add(x, y):\n",
    "    assert len(x.shape) == 2\n",
    "    assert x.shape == y.shape\n",
    "    \n",
    "    x = x.copy()\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            x[i, j] += y[i, j]\n",
    "            \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26e4f098",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_subtract(x, y):\n",
    "    assert len(x.shape) == 2\n",
    "    assert x.shape == y.shape\n",
    "    \n",
    "    x = x.copy()\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            x[i, j] -= y[i, j]\n",
    "            \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8fbcb6",
   "metadata": {},
   "source": [
    "같은 원리로 원소별 곱셈, 뺄셈 등을 할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ed8fe8",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a7a7ef",
   "metadata": {},
   "source": [
    "### 2.3.2 브로드캐스팅"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa082845",
   "metadata": {},
   "source": [
    "덧셈 구현인 `naive_add`는 동일한 크기의 2D Tensor의 덧셈을 지원함. 하지만 Dense층에서는 2D Tensor와 Vector를 더하였음. 크기가 다른 두 Tensor를 더했을때 **무슨 일이 일어날까?**\n",
    "\n",
    "* 모호하지 않고 실행 가능하다면 *작은* 텐서가 *큰* 텐서의 크기에 맞추어 **Broadcasting**됨\n",
    "    1. 큰 텐서의 ndim에 맞도록 작은 텐서에 축이 추가 됨 (**Broadcasting Axis**)\n",
    "    2. 작은 텐서가 새 축을 따라서 큰 텐서의 크기에 맞도록 반복됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1287ceeb",
   "metadata": {},
   "source": [
    "예시:\n",
    "\n",
    "* X의 크기는 (32, 10)이고 y의 크기는 (10,)라고 가정\n",
    "* y에 비어있는 첫번째 축을 추가하여 크기를 (1, 10)로 만듦\n",
    "* 그런 다음 y를 이 축에 32번 반복하면 텐서 Y의 크기는 (32, 10)이 됨\n",
    "* Y[i, :] == y for i in range(0, 32)\n",
    "* X와 Y는 크기가 같으므로 더할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a2e312",
   "metadata": {},
   "source": [
    "구현 입장에서는 새로운 텐서가 만들어지면 매우 **비효율적**이므로 어떤 2D Tensor로 만들어지지 않음. 반복된 연산은 가상적."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c713b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_add_matrix_and_vector(x, y):\n",
    "    assert len(x.shape) == 2\n",
    "    assert len(y.shape) == 1\n",
    "    assert x.shape[1] == y.shape[0]\n",
    "    \n",
    "    x = x.copy()\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            x[i, j] += y[j]\n",
    "            \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2404704c",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6f62a1",
   "metadata": {},
   "source": [
    "### 2.3.3 Tensor Product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b2d4012",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_vector_dot(x, y):\n",
    "    assert len(x.shape) == 1\n",
    "    assert len(y.shape) == 1\n",
    "    assert x.shape[0] == y.shape[0]\n",
    "    \n",
    "    z = 0\n",
    "    for i in range(x.shape[0]):\n",
    "        z += x[i] * y[i]\n",
    "        \n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b313b88",
   "metadata": {},
   "source": [
    "두 벡터의 점곱은 스칼라가 되므로 원소 개수가 같은 벡터끼리 점곱이 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1779e5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def naive_matrix_vector_dot(x, y):\n",
    "    assert len(x.shape) == 2\n",
    "    assert len(y.shape) == 1\n",
    "    assert x.shape[1] == y.shape[0]\n",
    "    \n",
    "    z = np.zeros(x.shape[0])\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            z[i] += x[i,j] * y[j]\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9215b3d9",
   "metadata": {},
   "source": [
    "행렬-벡터 점곱과 벡터-벡터 점곱 사이의 관계를 부각하기 위해 앞에서 만든 함수를 재활용 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34b7aae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_matrix_vector_dot(x, y):\n",
    "    z = np.zeros(x.shape[0])\n",
    "    for i in range(x.shape[0]):\n",
    "        z[i] = naive_vector_dot(x[i, :], y)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da328c6c",
   "metadata": {},
   "source": [
    "두 텐서 중 하나라도 ndim이 1보다 크면 dot product의 교환 법칙이 성립되지 않음\n",
    "* 벡터와 벡터간의 점곱은 교환 법칙이 성립하지만 행렬-벡터, 행렬-행렬은 성립하지 않음)\n",
    "* 즉, dot(x,y)와 dot(y,x)는 다름"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a6c4fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_matrix_dot(x, y):\n",
    "    assert len(x.shape) == 2\n",
    "    assert len(y.shape) == 2\n",
    "    assert x.shape[1] == y.shape[0]\n",
    "    \n",
    "    z = np.zeros((x.shape[0], y.shape[1]))\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(y.shape[1]):\n",
    "            row_x = x[i, :]\n",
    "            col_y = y[:, j]\n",
    "            z[i, j] = naive_vector_dot(row_x, col_y)\n",
    "            \n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0451faff",
   "metadata": {},
   "source": [
    "![](./images/2-3-3-dot.png)\n",
    "\n",
    "* x의 행 벡터와 y의 열 벡터가 같은 크기여야 함\n",
    "* 즉, x의 너비와 y의 높이는 동일함\n",
    "\n",
    "(a, b, c, d) . (d, ) -> (a, b, c)\n",
    "\n",
    "(a, b, c, d) . (d, e) -> (a, b, c, e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213d789b",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a382b0f",
   "metadata": {},
   "source": [
    "### 2.3.4 텐서 크기 변환 (Tensor Reshaping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059a0f3c",
   "metadata": {},
   "source": [
    "텐서의 크기를 변환한다는 것은 특정 크기에 맞게 열과 행을 *재배열*한다는 뜻"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6389881b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2)\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[0., 1.],\n",
    "              [2., 3.],\n",
    "              [4., 5.]])\n",
    "\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28334590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [3.],\n",
       "       [4.],\n",
       "       [5.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x.reshape((6, 1))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "554c524a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 2.],\n",
       "       [3., 4., 5.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x.reshape((2, 3))\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b356debe",
   "metadata": {},
   "source": [
    "자주 사용하는 특별한 크기 변환은 **Transpose** (전치)라고 불림. 행렬의 전치는 행과 열을 바꾸는 것. 즉, `x[i, :]`이 `x[:, i]`가 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d01b3b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 300)\n"
     ]
    }
   ],
   "source": [
    "x = np.zeros((300, 20))\n",
    "x = np.transpose(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed265bc4",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

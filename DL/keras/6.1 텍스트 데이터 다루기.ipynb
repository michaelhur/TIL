{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e40568b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "# Memory Pre-configuration\n",
    "config = tf.compat.v1.ConfigProto(\n",
    "    gpu_options=tf.compat.v1.GPUOptions(\n",
    "        per_process_gpu_memory_fraction=0.9,\n",
    "        allow_growth = True\n",
    "    )\n",
    "    # device_count = {'GPU': 1}\n",
    ")\n",
    "session = tf.compat.v1.Session(config=config)\n",
    "tf.compat.v1.keras.backend.set_session(session)\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f6c09f",
   "metadata": {},
   "source": [
    "시퀀스 데이터를 처리하는 기본적인 딥러닝 모델은 **순환 신경망 (Recurrent Neural Network)**과 **1D 컨브넷 (1D ConvNet)** 두 가지입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e117700",
   "metadata": {},
   "source": [
    "### 텍스트 데이터 다루기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75743ed4",
   "metadata": {},
   "source": [
    "텍스트는 가장 흔한 시퀀스 형태의 데이터입니다. 텍스트는 *단어의 시퀀스*나 *문자의 시퀀스*로 이해할 수 있습니다. 보통 *단어* 레벨에서 작업합니다. 시퀀스 처리용 딥러닝 모델은 문서 분류, 감성 분석, 저자 식별, 질문 응답 (Question Answering) 등의 애플리케이션에 적합합니다.\n",
    "\n",
    "다른 신경망 모델과 마찬가지로 텍스트 원본을 입력으로 사용하지 못합니다. 수치형 텐서로 변환 *(vectorizing text)*하는 여러가지 방식이 있습니다\n",
    "* 텍스트를 단어로 나누고 각 단어를 하나의 벡터로 변환합니다\n",
    "* 텍스트를 문자로 나누고 각 문자를 하나의 벡터로 변환합니다\n",
    "* 텍스트에서 단어나 문자의 **n-gram**을 추출하여 각 n그램을 하나의 벡터로 변환합니다\n",
    "\n",
    "텍스트를 나누는 이런 단위 (단어, 문자, n-그램)을 **토큰 (token)**이라고 합니다. 그리고 텍스트를 토큰으로 변환하는 작업을 **토큰화 (tokenization)**라고 합니다. 생성된 토큰에 수치형 벡터를 연결합니다. 이런 벡터는 시퀀스 텐서로 묶여져서 신경망에 주입됩니다.\n",
    "\n",
    "토근과 벡터를 연결하는 방법은 여러가지가 있습니다.\n",
    "* One-hot encoding\n",
    "* Token Embedding\n",
    "\n",
    "<br></br>\n",
    "![](../images/6-1-token.png)\n",
    "<br></br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d9182a",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5422eb4",
   "metadata": {},
   "source": [
    "### 단어와 문자의 원-핫 인코딩"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500d5754",
   "metadata": {},
   "source": [
    "단어에 고유한 정수 인덱스를 부여하고 이 정수 인덱스 i를 크기가 N (어휘 사전의 크기)인 이진 벡터로 변환합니다. 이 벡터는 i번째 인덱스만 1이고 나머지는 0입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9016c5",
   "metadata": {},
   "source": [
    "##### Word-level  One-hot encoding (example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5948135",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "## 초기 데이터: 각 문장이 샘플입니다.\n",
    "samples = [\"The cat sat on the mat.\", \"The dog at my homework.\"]\n",
    "\n",
    "## 데이터에 있는 모든 토큰의 인덱스를 구축합니다\n",
    "token_index = {}\n",
    "\n",
    "for sample in samples:\n",
    "    for word in sample.split():\n",
    "        if word not in token_index:\n",
    "            ## 단어마다 고유한 인덱스를 할당합니다. 인덱스 0은 사용하지 않습니다\n",
    "            token_index[word] = len(token_index) + 1\n",
    "\n",
    "##샘플을 벡터로 변환합니다. 각 샘플에서 max_length까지 단어만 사용합니다\n",
    "max_length = 10\n",
    "\n",
    "#결과를 저장할 배열입니다\n",
    "results = np.zeros(shape = (len(samples), max_length, max(token_index.values()) + 1))\n",
    "\n",
    "for i, sample in enumerate(samples):\n",
    "    for j, word in list(enumerate(sample.split()))[:max_length]:\n",
    "        index = token_index.get(word)\n",
    "        results[i, j, index] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ecd2934",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54512c1f",
   "metadata": {},
   "source": [
    "##### Char-level One-hot encoding (example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "505d35fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "samples = [\"The cat sat on the mat.\", \"The dog at my homework.\"]\n",
    "\n",
    "## 출력 가능한 모든 아스키(ASCII) 문자\n",
    "characters = string.printable \n",
    "\n",
    "token_index = dict(zip(characters, range(1, len(characters) + 1)))\n",
    "\n",
    "max_length = 50\n",
    "results = np.zeros((len(samples), max_length, max(token_index.values()) + 1))\n",
    "\n",
    "for i, sample in enumerate(samples):\n",
    "    for j, character in enumerate(sample):\n",
    "        index = token_index.get(character)\n",
    "        results[i, j, index] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a0017e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac08cb2",
   "metadata": {},
   "source": [
    "케라스에는 원본 덱스트 데이터를 단어 또는 문자 수준의 원-핫 인코딩으로 변환해주는 유틸리가 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f47028",
   "metadata": {},
   "source": [
    "##### 케라스를 사용한 단어 수준의 원-핫 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5268d525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9개의 고유한 토큰을 찾았습니다.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "samples = [\"The cat sat on the mat.\", \"The dog at my homework.\"]\n",
    "\n",
    "## 가장 빈도가 높은 1000개의 단어만 선택하도록 합니다\n",
    "tokenizer = Tokenizer(num_words = 1000)\n",
    "\n",
    "## 단어 인덱스를 구축합니다\n",
    "tokenizer.fit_on_texts(samples)\n",
    "\n",
    "## 문자열을 정수 인덱스의 리스트로 변환합니다.\n",
    "sequences = tokenizer.texts_to_sequences(samples)\n",
    "\n",
    "one_hot_results = tokenizer.texts_to_matrix(samples, mode = \"binary\")\n",
    "\n",
    "## 계산된 단어 인덱스를 구합니다\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "print(\"%s개의 고유한 토큰을 찾았습니다.\" % len(word_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c2c06e",
   "metadata": {},
   "source": [
    "원-핫 인코딩의 변종중 하나는 **원-핫 해싱 (one-hot hashing)** 기법입니다. \n",
    "* 어휘 사전에 있는 고유한 토큰의 수가 너무 커서 모두 다루기 힘들때 사용합니다. \n",
    "* 각 단어에 명시적으로 인덱스를 할당하고 이 인덱스를 딕셔너리에 저장하는 대신에 단어를 해싱하여 고정된 크기의 벡터로 변환합니다. \n",
    "* 장점은 명시적인 단어 인덱스가 필요 없기때문에 메모리를 절약하고\n",
    "* 전체 데이터를 확인하지 않고 토큰을 생성할 수 있습니다.\n",
    "* 단점은 해시 충돌입니다\n",
    "* 두개의 단어가 같은 해시를 만들면 머신 러닝 모델은 두 단어의 차이를 인식하지 못합니다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf15c01",
   "metadata": {},
   "source": [
    "##### 해싱 기법을 사용한 단어 수준의 원-핫 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d89f2963",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [\"The cat sat on the mat.\", \"The dog at my homework.\"]\n",
    "\n",
    "## 단어를 크기가 1000인 벡터에 저장합니다\n",
    "dimensionality = 1000\n",
    "max_length = 10\n",
    "\n",
    "results = np.zeros((len(samples), max_length, dimensionality))\n",
    "for i, sample in enumerate(samples):\n",
    "    for j, word in list(enumerate(sample.split()))[:max_length]:\n",
    "        index = abs(hash(word)) % dimensionality\n",
    "        results[i, j, index] = 1.\n",
    "\n",
    "    results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6b7e48",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda454b0",
   "metadata": {},
   "source": [
    "### 단어 임베딩 사용하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f105571c",
   "metadata": {},
   "source": [
    "단어와 벡터를 연결시키는 인기있는 방법은 **단어 임베딩**이라는 밀집 단어 벡터를 사용하는 것입니다.\n",
    "* 원-핫 인코딩으로 만들어진 벡터는 희소 (sparse)하고 고차원입니다. (어휘 사전에 있는 단어의 수와 차원이 같습니다)\n",
    "* 반면에 단어 임베딩은 저차원의 실수형 벡터입니다\n",
    "* 원-핫 인코딩과 달리 단어 임베딩은 데이터로부터 학습됩니다\n",
    "* 단어 임베딩이 더 많은 정보를 적은 차원에 저장합니다\n",
    "\n",
    "<br></br>\n",
    "![](../images/6-1-wordembed.png)\n",
    "<br></br>\n",
    "\n",
    "단어 임베딩을 만드는 방법은 2가지 입니다.\n",
    "* (문서 분류나 감성 예측같은) 관심 대상인 문제와 함께 단어 임베딩을 학습합니다. 이 경우에는 랜덤한 단어 벡터로 시작해서 신경망의 가중치를 학습하는 것과 같은 방식으로 단어 벡터를 학습합니다.\n",
    "* 미리 계산된 단어 임베딩을 로드합니다. 이를 **사전 훈련된 단어 임베딩 (pretrained word embedding)**이라고 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea6eee7",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f19259",
   "metadata": {},
   "source": [
    "#### Embedding 층을 사용하여 단어 임베딩 학습하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c74bd8",
   "metadata": {},
   "source": [
    "단어와 밀집 벡터를 연관짓는 가장 간단한 방법은 랜덤하게 벡터를 선택하는 것입니다.\n",
    "* 문제점은 임베딩 공간이 구조적이지 않다는 것\n",
    "* accurate와 exact는 대부분 문장에서 비슷한 의미로 사용되지만 다른 임베딩을 가집니다\n",
    "* 심층 신경망은 이런 임의의 구조적이지 않은 임베딩 공간을 이해하기 어렵습니다\n",
    "\n",
    "단어 벡터 사이에 추상적이고 기하학적인 관계를 얻으려면 단어 사이에 있는 의미 관계를 반영해야합니다.\n",
    "* 단어 임베딩은 언어를 기하학적 공간에 매핑하는 것입니다.\n",
    "* 잘 구축된 임베딩 공간에서는 동의어가 비슷한 단어 벡터로 임베딩될 것입니다.\n",
    "* 두 단어 사이의 거리 (L2 거리)는 단어 사이의 의미 거리와 관계되어 있습니다\n",
    "    * 가까운 거리의 단어는 비슷한 뜻을 가진 단어\n",
    "    * 멀리 떨어진 단어는 다른 의미의 단어\n",
    "* 거리뿐만 아니라 방향도 의미를 가질 수 있습니다.\n",
    "\n",
    "실제 단어 임베딩 공간에서 의미있는 기하학적 변환의 일반적인 예는 **성별**과 **복수 (plural)** 벡터입니다.\n",
    "* 예를 들어 *king* 벡터에 *female* 벡터를 더하면 *queen* 벡터가 됩니다.\n",
    "* *king* 벡터에 *plural* 벡터를 더하면 *kings* 벡터가 됩니다.\n",
    "* 단어 임베딩 공간은 전형적으로 이런 해석 가능하고 잠재적으로 유용한 수천개의 벡터를 특성으로 가집니다.\n",
    "\n",
    "영어로 된 영화 리뷰 감성 모델을 위한 단어 임베딩 공간은 영어로 된 법률 문서 분류 모델을 위한 임베딩 공간과 다릅니다. 특정 의미 관계의 중요성이 작업에 따라 다르기 떄문입니다.\n",
    "\n",
    "따라서 새로운 작업에는 새로운 임베딩을 학습하는 것이 타당합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e32772",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e657e421",
   "metadata": {},
   "source": [
    "##### Embedding 층의 객체 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "390d4824",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "\n",
    "## Embedding 층은 적어도 2개의 매개변수를 받습니다\n",
    "## 가능한 토큰의 수 (1000)와 임베딩 차원 (64)입니다\n",
    "embedding_layer = Embedding(1000, 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a972d4bd",
   "metadata": {},
   "source": [
    "Embedding Layer는 크기가 (samples, sequence_length)인 2D 정수 텐서를 입력으로 받습니다.\n",
    "* 각 샘플은 정수의 시퀀스입니다.\n",
    "* 배치에 있는 모든 시퀀스는 길이가 같아야 합니다.\n",
    "* 작은 길이의 시퀀스는 0으로 패딩되고 길이가 긴 시퀀스는 잘립니다.\n",
    "\n",
    "Embedding Layer는 크기가 (samples, sequence_length, embedding_dimensionality)인 3D 실수형 텐서를 반환합니다.\n",
    "* 이런 3D 텐서는 RNN 층이나 2D 합성곱 층에서 처리됩니다\n",
    "\n",
    "Embedding Layer의 객체를 생성할때 가중치는 다른 층과 마찬가지로 랜덤하게 초기화됩니다. 훈련하면서 이 단어 벡터는 역전파를 통해 점차 조정되어 이어지는 모델이 사용할 수 있도록 임베딩 공간을 구성합니다. 훈련이 끝나면 임베딩 공간은 특정 문제에 특화된 구조를 많이 가지게 됩니다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd831605",
   "metadata": {},
   "source": [
    "##### Embedding Layer에 사용할 IMDB 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6eb8b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<__array_function__ internals>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "/home/jhhur/anaconda3/envs/TIL/lib/python3.8/site-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "/home/jhhur/anaconda3/envs/TIL/lib/python3.8/site-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras import preprocessing\n",
    "\n",
    "## 특성으로 사용할 단어의 수\n",
    "max_features = 10000\n",
    "\n",
    "## 사용할 텍스트의 길이\n",
    "maxlen = 20\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words = max_features)\n",
    "\n",
    "x_train = preprocessing.sequence.pad_sequences(x_train, maxlen = maxlen)\n",
    "x_test = preprocessing.sequence.pad_sequences(x_test, maxlen = maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64acbbbd",
   "metadata": {},
   "source": [
    "##### IMDB 데이터에 Embedding 층과 분류기 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20edb381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 1s 54us/sample - loss: 0.6744 - acc: 0.6124 - val_loss: 0.6311 - val_acc: 0.6958\n",
      "Epoch 2/10\n",
      " 3168/20000 [===>..........................] - ETA: 0s - loss: 0.6036 - acc: 0.7380"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jhhur/anaconda3/envs/TIL/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 1s 36us/sample - loss: 0.5559 - acc: 0.7462 - val_loss: 0.5330 - val_acc: 0.7294\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 1s 34us/sample - loss: 0.4671 - acc: 0.7883 - val_loss: 0.5002 - val_acc: 0.7438\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 1s 34us/sample - loss: 0.4220 - acc: 0.8102 - val_loss: 0.4925 - val_acc: 0.7466\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 1s 35us/sample - loss: 0.3909 - acc: 0.8268 - val_loss: 0.4919 - val_acc: 0.7592\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 1s 38us/sample - loss: 0.3660 - acc: 0.8393 - val_loss: 0.4963 - val_acc: 0.7578\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 1s 40us/sample - loss: 0.3435 - acc: 0.8516 - val_loss: 0.5030 - val_acc: 0.7536\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 1s 40us/sample - loss: 0.3233 - acc: 0.8632 - val_loss: 0.5105 - val_acc: 0.7518\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 1s 38us/sample - loss: 0.3046 - acc: 0.8734 - val_loss: 0.5197 - val_acc: 0.7486\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 1s 36us/sample - loss: 0.2872 - acc: 0.8815 - val_loss: 0.5290 - val_acc: 0.7508\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Embedding\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(10000, 8, input_length = maxlen))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "model.compile(optimizer = 'rmsprop', loss = 'binary_crossentropy', metrics = ['acc'])\n",
    "\n",
    "history = model.fit(x_train, y_train, epochs = 10, batch_size = 32, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5de579",
   "metadata": {},
   "source": [
    "약 75%의 검증 정확도가 나옵니다. 하지만 임베딩 시퀀스를 펼치고 하나의 Dense 층을 훈련했으므로 입력 시퀀스에 있는 각 단어를 독립적으로 다루었습니다. 단어 사이의 관계나 문장 구조를 고려하지 않았습니다. 각 시퀀스 전체를 고려한 특성을 학습하도록 임베딩 층 위에 순환 층이나 1D 합성곱 층을 추가하는 것이 좋습니다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5457aaf4",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2542c339",
   "metadata": {},
   "source": [
    "#### 사전 훈련된 단어 임베딩 사용하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808b0702",
   "metadata": {},
   "source": [
    "훈련 데이터가 부족하면 작업에 맞는 단어 임베딩을 학습할 수 없습니다. 이때는 어떻게 해야할까요?\n",
    "\n",
    "풀려는 문제와 함께 담어 임베딩을 학습하는 대신 미리 계산된 임베딩 공간에서 임베딩 벡터를 로드할 수 있습니다. 이런 임베딩 공간은 뛰어난 구조와 유용한 성질을 가지고 있어서 언어 구조의 일반적인 측면을 잡아낼 수 있습니다. 충분한 데이터가 없어서 자신만의 좋은 특성을 학습하지 못하지만 꽤 일반적인 특성이 필요할 때입니다. 이런 경우에는 다른 문제에서 학습한 특성을 재사용하는 것이 합리적입니다.\n",
    "\n",
    "단어 임베딩은 일반적으로 단어 출현 통계를 사용하여 계산합니다. 단어를 위해 밀집된 저차원 임베딩 공간을 비지도 학습 방법으로 계산하는 아이디어가 적용되었습니다\n",
    "* 2013년 구글이 개발한 Word2vec은 성별처럼 구체적인 의미가 있는 속성을 잡아냅니다\n",
    "* 2014년 스탠포드에서 개발한 Glove는 단어의 동시 출현 통계를 기록한 행렬을 분해하는 방법을 사용합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f8732c",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a900bc8",
   "metadata": {},
   "source": [
    "### 모든 내용을 적용하기: 원본 텍스트에서 단어 임베딩까지"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734fc0e0",
   "metadata": {},
   "source": [
    "문장들을 벡터의 시퀀스로 임베딩하고 펼친 후 그 위에 Dense층을 훈련합니다. 여기서는 사전 훈련된 단어 임베딩을 사용하겠습니다. 케라스에 포함된 IMDB 데이터는 미리 토큰화가 되어있어서 이를 사용한 대신 원본 텍스트 데이터를 내려받아 처음부터 시작하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ce34a0",
   "metadata": {},
   "source": [
    "#### 원본 IMDB 텍스트 내려받기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f7e5f9",
   "metadata": {},
   "source": [
    "##### IMDB 원본 데이터 전처리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "63cfd362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/aclImdb/train/neg\n",
      "./data/aclImdb/train/pos\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "imdb_dir = \"./data/aclImdb/\"\n",
    "train_dir = os.path.join(imdb_dir, \"train\")\n",
    "\n",
    "labels = []\n",
    "texts = []\n",
    "\n",
    "for label_type in [\"neg\", \"pos\"]:\n",
    "    dir_name = os.path.join(train_dir, label_type)\n",
    "    print(dir_name)\n",
    "    \n",
    "    for fname in os.listdir(dir_name):\n",
    "        if fname[-4:] == \".txt\":\n",
    "            f = open(os.path.join(dir_name, fname), encoding = \"utf8\")\n",
    "            texts.append(f.read())\n",
    "            f.close()\n",
    "            \n",
    "            if label_type == \"neg\":\n",
    "                labels.append(0)\n",
    "                \n",
    "            else:\n",
    "                labels.append(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bd1121",
   "metadata": {},
   "source": [
    "#### 데이터 토큰화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead8680d",
   "metadata": {},
   "source": [
    "텍스트를 벡터로 만들고 훈련 세트와 검증 세트로 나누겠습니다. 사전 훈련된 단어 임베딩은 훈련 데이터가 부족한 문제에 특히 유용합니다. 그래서 다음과 같이 훈련 데이터를 처음 200개의 샘플로 제한합니다. 이 모델은 200개의 샘플을 학습한 후 영화 리뷰를 분류할 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5e47e1",
   "metadata": {},
   "source": [
    "##### IMDB 원본 데이터의 텍스트를 토큰화하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c3790497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88582개의 고유한 토큰을 찾았습니다\n",
      "데이터 텐서의 크기: (25000, 100)\n",
      "레이블 텐서의 크기: (25000,)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "maxlen = 100\n",
    "training_samples = 200\n",
    "validation_samples = 10000\n",
    "max_words = 10000\n",
    "\n",
    "tokenizer = Tokenizer(num_words = max_words)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print(\"%s개의 고유한 토큰을 찾았습니다\" % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen = maxlen)\n",
    "labels = np.asarray(labels)\n",
    "\n",
    "print(\"데이터 텐서의 크기:\", data.shape)\n",
    "print(\"레이블 텐서의 크기:\", labels.shape)\n",
    "\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "x_train = data[:training_samples]\n",
    "y_train = labels[:training_samples]\n",
    "\n",
    "x_val = data[training_samples: training_samples + validation_samples]\n",
    "y_val = labels[training_samples: training_samples + validation_samples]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760db914",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28bc66e",
   "metadata": {},
   "source": [
    "#### 임베딩 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9549c742",
   "metadata": {},
   "source": [
    "압축 해제한 파일을 파싱하여 단어와 이에 상응하는 벡터 표현을 매칭하는 인덱스를 만듭니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63171b33",
   "metadata": {},
   "source": [
    "##### Glove 단어 임베딩 파일 파싱하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0ed67235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000개의 단어 벡터를 찾았습니다\n"
     ]
    }
   ],
   "source": [
    "glove_dir = \"./data/\"\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(glove_dir, \"glove.6B.100d.txt\"), encoding = \"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype = \"float32\")\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print(\"%s개의 단어 벡터를 찾았습니다\" % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f521d5",
   "metadata": {},
   "source": [
    "그 다음 Embedding 층에 주입할 수 있도록 임베딩 행렬을 만듭니다. \n",
    "* 크기는 **(max_words, embedding_dim)**\n",
    "* 행렬의 i번째 원소는 단어 인덱스의 i번째 단어에 상응하는 embedding_dim차원 벡터입니다\n",
    "* 인덱스 0은 어떤 단어나 토큰도 아닐 경우를 나타냅니다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff3b9b0",
   "metadata": {},
   "source": [
    "##### Glove 단어 임베딩 행렬 준비하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fe2dbc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i < max_words:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e656159",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6c57c0",
   "metadata": {},
   "source": [
    "#### 모델 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "57cbabbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 100, 100)          1000000   \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 10000)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                320032    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,320,065\n",
      "Trainable params: 1,320,065\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_dim, input_length = maxlen))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, activation = \"relu\"))\n",
    "model.add(Dense(1, activation = \"sigmoid\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f22c0f",
   "metadata": {},
   "source": [
    "#### 모델에 GloVe 임베딩 로드하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02eeab70",
   "metadata": {},
   "source": [
    "Embedding 층은 하나의 가중치 행렬을 가집니다. 이 행렬은 2D 행렬이고 각 i번째 원소는 i번째 인덱스에 상응하는 단어 벡터입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88299d3e",
   "metadata": {},
   "source": [
    "##### 사전 훈련된 단어 임베딩을 Embedding 층에 로드하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "62554e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[0].set_weights([embedding_matrix])\n",
    "model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f18d7c2",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

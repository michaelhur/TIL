{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3e1c139",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "# Memory Pre-configuration\n",
    "config = tf.compat.v1.ConfigProto(\n",
    "    gpu_options=tf.compat.v1.GPUOptions(\n",
    "        per_process_gpu_memory_fraction=0.9,\n",
    "        allow_growth = True\n",
    "    )\n",
    "    # device_count = {'GPU': 1}\n",
    ")\n",
    "session = tf.compat.v1.Session(config=config)\n",
    "tf.compat.v1.keras.backend.set_session(session)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24714bbf",
   "metadata": {},
   "source": [
    "###  고급 구조 패턴"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0063167f",
   "metadata": {},
   "source": [
    "#### 배치 정규화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c1d99c",
   "metadata": {},
   "source": [
    "**정규화 (normalization)**는 머신 러닝 모델에 주입되는 샘플들을 균일하게 만드는 광범위한 방법입니다.\n",
    "* 모델이 학습하고 새로운 데이터에 잘 일반화되도록 돕습니다\n",
    "* 데이터가 정규 분포를 다른다 가정합니다\n",
    "\n",
    "`nomarlized_data = (data - np.mean(data, axis = ...)) / np.std(data, axis = ...)`\n",
    "\n",
    "이전 예제에서는 모델에 데이터를 주입하기 전에 정규화했습니다. 하지만 데이터 정규화는 네트워크에서 일어나는 모든 변환 후에도 고려되어야 합니다. \n",
    "* Dense나 Conv2D 층에 들어가는 데이터의 평균이 0이고 분산이 1이더라도 출력되는 데이터가 동일한 분포를 가질 것이라고 기대하기 어렵습니다\n",
    "\n",
    "**배치 정규화 (batch normalization)**은 훈련하는 동안 평균과 분산이 바뀌더라도 이에 적응하여 데이터를 정규화합니다. \n",
    "* 훈련 과정에서 사용된 배치 데이터의 평균과 분산에 대한 *지수 이동 평균 (exponential moving average)*을 내부에 유지\n",
    "* 배치 정규화의 주요 효과는 잔차 연결과 흡사해 그래디언트의 전파를 도와주는 것\n",
    "* 결국 더 깊은 네트워크를 구성할 수 있음\n",
    "\n",
    "Batch Normalization 층은 일반적으로 합성곱이나 완전 연결 층 다음에 사용합니다\n",
    "\n",
    "```python\n",
    "conv_model.add(layers.Conv2D(32, 3, activation = \"relu\"))\n",
    "conv_model.add(layers.BatchNormalization())\n",
    "\n",
    "dense_model.add(layers.Dense(32, actiation = \"relu\"))\n",
    "dense_model.add(layers.BatchNormalization())\n",
    "```\n",
    "\n",
    "BatchNormalization 클래스에는 정규화할 특성 축을 지정하는 axis 매개변수가 있습니다. \n",
    "* 이 매개변수의 기본값은 입력 텐서의 *마지막 축*을 나타내는 -1 입니다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2864b3",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102b5285",
   "metadata": {},
   "source": [
    "#### 깊이별 분리 합성곱"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ec8bd7",
   "metadata": {},
   "source": [
    "**깊이별 분리 합성곱 (depthwise separable convolution) 층**은 Conv2D를 대체하면서 더 가볍고 (모델 파라미터가 적고) 더 빠른 층입니다\n",
    "* 채널별로 따로따로 공간 방향의 합성곱을 수행\n",
    "* 그 다음 점별 합성곱 (1x1 합성곱)을 통해 출력 채널을 합침\n",
    "* *공간 특성의 학습*과 *채널 방향 특성의 학습*을 분리하는 효과를 냄\n",
    "* 입력에서 공간상 위치는 상관관계가 크지만 채널별로는 매우 독립적이라고 가정한다면 타당함\n",
    "* 이 방법은 모델 파라미터와 연산의 수를 크게 줄여주기 때문에 작고 더 빠른 모델을 만듬\n",
    "* 합성곱을 통해 더 효율적으로 표현을 학습하기 떄문에 적은 데이터로도 더 좋은 표현을 학습함\n",
    "* 제한된 데이터로 작은 모델을 처음부터 훈련시킬때 특히 더 중요\n",
    "\n",
    "<br></br>\n",
    "![](../images/7-2-separablecnn.png)\n",
    "<br></br>\n",
    "\n",
    "다음은 작은 데이터셋에서 이미지 분류 문제를 위한 가벼운 깊이별 분리 컨브넷을 만드는 예시입니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07aef3a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "separable_conv2d (SeparableC (None, 62, 62, 32)        155       \n",
      "_________________________________________________________________\n",
      "separable_conv2d_1 (Separabl (None, 60, 60, 64)        2400      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 30, 30, 64)        0         \n",
      "_________________________________________________________________\n",
      "separable_conv2d_2 (Separabl (None, 28, 28, 64)        4736      \n",
      "_________________________________________________________________\n",
      "separable_conv2d_3 (Separabl (None, 26, 26, 128)       8896      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 128)       0         \n",
      "_________________________________________________________________\n",
      "separable_conv2d_4 (Separabl (None, 11, 11, 64)        9408      \n",
      "_________________________________________________________________\n",
      "separable_conv2d_5 (Separabl (None, 9, 9, 128)         8896      \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 38,949\n",
      "Trainable params: 38,949\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras import layers\n",
    "\n",
    "height = 64\n",
    "width = 64\n",
    "channels = 3\n",
    "num_classes = 10\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.SeparableConv2D(32, 3,\n",
    "                                 activation = \"relu\",\n",
    "                                 input_shape = (height, width, channels, )))\n",
    "model.add(layers.SeparableConv2D(64, 3, activation = \"relu\"))\n",
    "model.add(layers.MaxPooling2D(2))\n",
    "\n",
    "model.add(layers.SeparableConv2D(64, 3, activation = \"relu\"))\n",
    "model.add(layers.SeparableConv2D(128, 3, activation = \"relu\"))\n",
    "model.add(layers.MaxPooling2D(2))\n",
    "\n",
    "model.add(layers.SeparableConv2D(64, 3, activation = \"relu\"))\n",
    "model.add(layers.SeparableConv2D(128, 3, activation = \"relu\"))\n",
    "model.add(layers.GlobalAveragePooling2D())\n",
    "\n",
    "model.add(layers.Dense(32, activation = \"relu\"))\n",
    "model.add(layers.Dense(num_classes, activation = \"softmax\"))\n",
    "\n",
    "model.compile(optimizer = \"rmsprop\",\n",
    "              loss = \"categorical_crossentropy\")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151b3afc",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2322584",
   "metadata": {},
   "source": [
    "### 하이퍼파라미터 최적화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd073208",
   "metadata": {},
   "source": [
    "딥러닝 모델을 만들때 무작위로 보이는 결정을 많이 해야합니다\n",
    "* 얼마나 많은 층을 쌓아야 할까요?\n",
    "* 층마다 얼마나 많은 유닛이나 필터를 두어야 할까요?\n",
    "* relu 활성화 함수를 사용해야 할까요?\n",
    "* 어떤 층 뒤에 Batch Normalization을 사용해야 할까요?\n",
    "* Dropout은 얼마나 사용해야할까요?\n",
    "\n",
    "이런 구조에 관련된 파라미터를 **하이퍼파라미터**라고 부릅니다.\n",
    "\n",
    "전형적인 하이퍼파라미터 최적화 과정은 다음과 같습니다\n",
    "1. 일련의 하이퍼파라미터를 선택합니다\n",
    "2. 선택된 하이퍼파라미터로 모델을 만듭니다\n",
    "3. 훈련 데이터에 학습하고 검증 데이터에서 최종 성능을 측정합니다\n",
    "4. 다음으로 시도할 하이퍼파라미터를 선택합니다\n",
    "5. 이 과정을 반복합니다\n",
    "6. 마지막으로 테스트 데이터에서 성능을 측정합니다\n",
    "\n",
    "주어진 하이퍼파라미터에서 얻은 검증 성능을 사용하여 다음번에 시도할 하이퍼파라미터를 선택하는 알고리즘이 이 과정의 핵심입니다\n",
    "* 베이지안 최적화 (Bayesian Optimization)\n",
    "* 유전 알고리즘 (Genetic Algorithms)\n",
    "* 간단한 랜덤 탐색 (Random Search)\n",
    "등이 있습니다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc6f36e",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35371c7d",
   "metadata": {},
   "source": [
    "### 모델 앙상블 (Ensemble)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89eccae7",
   "metadata": {},
   "source": [
    "**모델 앙상블 (Model Ensemble)**은 가장 좋은 결과를 얻을 수 있는 또 다른 방법입니다. 앙상블은 여러개 다른 모델의 예측을 합쳐서 더 좋은 예측을 만듭니다.\n",
    "\n",
    "앙상블은 독립적으로 훈련된 다른 종류의 좋은 모델이 각기 다른 장점을 가지고 있다는 가정을 바탕으로 합니다. \n",
    "* 각 모델은 예측을 만들기 위해 조금씩 다른 측면을 바라봅니다\n",
    "* 데이터의 부분 특징입니다\n",
    "* 각자의 가정 (고유한 모델 구조와 랜덤 가중치 초기화)을 이용하고 각자의 관점으로 이해합니다\n",
    "* 각 모델은 데이터의 일부분에 맞는 정답을 찾지만 완전한 정답은 아닙니다.\n",
    "* 이들의 관점을 모으면 데이터를 훨씬 더 정확하게 묘사할 수 있습니다.\n",
    "\n",
    "분류의 예를 들어보면 분류기 예측을 합치는 가장 쉬운 방법은 추론할때 나온 예측을 평균 내는 것입니다.\n",
    "\n",
    "```python\n",
    "preds_a = model_a.predict(x_val)\n",
    "preds_b = model_b.predict(x_val)\n",
    "preds_c = model_c.predict(x_val)\n",
    "preds_d = model_d.predict(x_val)\n",
    "\n",
    "final_preds = 0.25 * (preds_a + preds_b + preds_c + preds_d)\n",
    "```\n",
    "\n",
    "분류기를 앙상블하는 좋은 방법은 검증 데이터에서 학습된 가중치를 사용하여 가중 평균하는 것입니다. 전형적으로 분류기가 좋은수록 높은 가중치를 가지고 나쁜 분류기일수록 낮은 가중치를 갖습니다.\n",
    "\n",
    "앙상블이 잘 작동하게 만드는 핵심은 *분류기의 다양성*입니다. 모든 모델이 같은 방향으로 편향되어 있다면 앙상블은 동일한 편향을 유지할 것입니다. 모델이 서로 다른 방향으로 편향되어 있다면 편향은 서로 상쇄되고 앙상블이 더 견고하고 정확해집니다.\n",
    "\n",
    "이런 이유때문에 가능한 최대한 다르면서 좋은 모델을 앙상블해야 합니다. 일반적으로 매우 다른 구조를 가지거나 다른 종류의 머신 러닝 방법을 말합니다.\n",
    "* 같은 네트워크를 랜덤 초기화를 다르게 하여 여러번 훈련해서 앙상블하는 것은 거의 해볼 가치가 없습니다\n",
    "* 모델 사이 차이점이 랜덤 초기화와 모델에 주입되는 훈련 데이터의 순서라면 이 앙상블은 다양성이 낮고 하나의 모델보다 아주 조금만 성능이 향상됩니다"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

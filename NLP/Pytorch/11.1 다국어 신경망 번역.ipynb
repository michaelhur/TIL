{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4da483b",
   "metadata": {},
   "source": [
    "### Zero shot Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9966d8d",
   "metadata": {},
   "source": [
    "이 방식의 특징은 여러 언어쌍의 병렬 코퍼스를 하나의 모델에 훈련하면 부가적으로 학습에 참여한 코퍼스에 존재하지 않는 언어쌍도 번역이 가능하다는 점입니다. 즉 한번도 기계번역 모델에게 데이터를 보여주지 않아도 해당 언어쌍 번역을 처리할 수 있으므로 **제로샷 학습**이라는 이름이 붙었습니다.\n",
    "\n",
    "구현 방법은 매우 간단합니다. 다음과 같이 기존 병렬 코퍼스의 맨 앞에 특수 토큰을 삽입하고 훈련함으로써 완성이 됩니다. 삽입된 토크에 따라 타깃 언어가 결정됩니다.\n",
    "\n",
    "<br></br>\n",
    "\n",
    "| |소스 언어|타깃 언어|\n",
    "|-|-------|-------|\n",
    "|기존|Hello, How are you?|Hola, como estas?|\n",
    "|제로샷|<2es> Hello, How are you?|Hola, como estas?|\n",
    "\n",
    "<br></br>\n",
    "\n",
    "실험의 목표는 단순히 다국어 신경망 머신러닝의 end-to-end 모델을 구현하는 것뿐만 아니라, 서로 다른 언어쌍의 코퍼스를 활용하여 번역기의 모든 언어쌍에 대해 전체적인 성능을 올릴 수 있는지를 확인하려는 관점도 있습니다. 이에 따라 실험은 크게 4가지 관점에서 수행되었습니다.\n",
    "\n",
    "<br></br>\n",
    "\n",
    "|번호|방법|설명|\n",
    "|---|---|---|\n",
    "|1|다수의 언어에서 하나의 언어로|다수의 언어를 인코더에 넣고 훈련시킵니다|\n",
    "|2|하나의 언어에서 다수의 언어로|다수의 언어를 디코더에 넣고 훈련시킵니다|\n",
    "|3|다수의 언어에서 다수의 언어로|다수의 언어를 인코더와 디코더에 모두 넣고 훈련시킵니다|\n",
    "|4|제로샷 번역|앞의 방법으로 훈련된 모델에서 훈련 코퍼스에 존재하지 않던 언어 쌍의 번역 성능을 평가합니다|\n",
    "\n",
    "<br></br>\n",
    "\n",
    "언어가 서로 다른 코퍼스를 하나로 합치다 보면 분량이 다르므로 이에 대한 대처 방법도 정의해야합니다. 따라서 다음 실험들에서는 oversampling 깁버을 사용하여 양이 적은 코퍼스를 양이 많은 코퍼스와 비슷해지도록 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00befe7",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b80419",
   "metadata": {},
   "source": [
    "#### Many to One"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ddd0c9",
   "metadata": {},
   "source": [
    "이 실험에서는 전체적으로 성능이 현상된 것을 볼 수 있습니다. 실제로 문제로 주어진 언어 데이터셋 외에도, 동시에 훈련된 다른 언어의 데이터셋을 통해 해당 언어의 번역 성능을 높이는 정보를 추가로 얻을 수 있습니다.\n",
    "\n",
    "<br></br>\n",
    "![](./images/11-1-1-manytoone.jpg)\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ab8ce9",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f6f566",
   "metadata": {},
   "source": [
    "#### One to Many"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42de0f9",
   "metadata": {},
   "source": [
    "이 실험에서는 이전 실험과 달리 성능의 향상이 있다고 보기 어렵습니다. 게다가 오버샘플링과 관련해서 코퍼스의 양이 적은 영어-독일어 언어쌍 코퍼스는 오버샘플링의 이득을 본 반면, 양이 충분한 영어-프랑스어 언어쌍 코퍼스의 경우에는 오버 샘플링을 하면 더 큰 손해를 보는 것을 알 수 있습니다.\n",
    "\n",
    "<br></br>\n",
    "![](./images/11-1-1-onetomany.jpg)\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc3ec57",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dde3299",
   "metadata": {},
   "source": [
    "#### Many to Many"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3716112a",
   "metadata": {},
   "source": [
    "대부분의 실험 결과가 성능의 하락으로 이어졌습니다.\n",
    "\n",
    "<br></br>\n",
    "![](./images/11-1-1-manytomany.jpg)\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a1e1d3",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5121412",
   "metadata": {},
   "source": [
    "#### Zero Shot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febb5348",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "![](./images/11-1-1-zeroshot.jpg)\n",
    "<br></br>\n",
    "\n",
    "이 테이블의 $브릿지^{bridge}$ 방법은 중간 언어를 영어로 한다면 포르투갈어 -> 영어 -> 스페인어와 같이 2단계에 걸쳐 번역합니다. \n",
    "\n",
    "$구문\\ 기반\\ 기계번역^{phrase-based\\ machine\\ translation}$ (PBMT) 방식은 통계 기반 기계번역 (SMT) 중 하나입니다.\n",
    "\n",
    "(c)의 **\"NMT 포르투갈어 -> 스페인어\"**는 단순 병렬 코퍼스를 활용하여 기존 방법대로 훈련한 베이스라인입니다.\n",
    "\n",
    "(d)의 **\"모델1\"**은 포르투갈어 -> 영어, 영어 -> 스페인어를 1개의 모델에 훈련한 버전입니다.\n",
    "\n",
    "(e)의 **\"모델2\"**는 영어 <-> 포르투갈어, 영어 <-> 스페인어 코퍼스를 한 모델에 훈련한 버전입니다. 이때 총 4가지 코퍼스를 훈련했다는 점에 주의해야합니다.\n",
    "\n",
    "(f)의 **\"모델2 + 점진 학습 방식\"**은 (c)보다 적은 양의 병렬 코퍼스로 훈련한 기존 모델에 추가로 모델 2방식으로 훈련한 모델입니다.\n",
    "\n",
    "비록 \"모델1\"과 \"모델2\"는 훈련 중에는 한번도 포르투갈어 -> 스페인어 병렬 코퍼스를 보지 못했지만 20이 넘는 BLEU를 보여줍니다. (f)의 경우에는 (c)와 큰 차이는 아니지만 성능이 뛰어남을 알 수 있습니다. 따라서 병렬 코퍼스의 양이 얼마되지 않는 언어쌍의 번역기를 훈련할때는 이러한 방법을 통해 성능을 끌어올릴 수 있습니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

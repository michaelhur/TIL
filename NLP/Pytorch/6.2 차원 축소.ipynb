{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03a4ae30",
   "metadata": {},
   "source": [
    "단어의 의미를 다루면서, 코퍼스로부터 단어의 $특징^{feature}$를 추출하여 벡터로 만드는 과정을 살펴보았습니다. 하지만 차원의 저주로 인해 그 결과물은 여전히 희소 벡터로 나타남을 알았습니다. 같은 데이터를 표현할때 가능한 낮은 차원으로 표현할수록 쉽게 모델링하고 학습할 수 있으므로, 이러한 희소 벡터로 나타나는 것보다는 **$덴스^{dense}$** 벡터로 표현해주는 것이 훨씬 좋을 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9510dee3",
   "metadata": {},
   "source": [
    "### 차원 축소 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fa6d62",
   "metadata": {},
   "source": [
    "이전 장에서는 높은 차원에서 데이터를 표현하는 과정에서 희소성 문제가 많이 나타남을 확인했습니다. 따라서 같은 정보를 표현할때는 더 낮은 차원을 사용하는 것이 중요합니다.\n",
    "\n",
    "<br></br>\n",
    "![](./images/6-1-1-dimension.jpg)\n",
    "<br></br>\n",
    "\n",
    "이번 장에서는 더 작은 차원으로 효율적으로 정보를 표현하는 차원 축소의 이유와 방법에 관해 다루어보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f508a1",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf6b5ad",
   "metadata": {},
   "source": [
    "### 주성분 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22ff82a",
   "metadata": {},
   "source": [
    "대표적인 차원 축소 방법으로는 **$주성분\\ 분석^{principal\\ component\\ analysis}(PCA)$**가 있습니다.\n",
    "\n",
    "<br></br>\n",
    "![](./images/6-1-2-pca.jpg)\n",
    "<br></br>\n",
    "\n",
    "이와 같이 고차원의 데이터를 더 낮은 차원으로 표현할 수 있습니다. 주로 **$특잇값\\ 분해^{singular\\ value\\ decomposition}(SVD)$**를 통해 주성분을 분석할 수 있습니다. 이때 축소를 위한 주성분은 다음과 같은 조건을 만족합니다.\n",
    "\n",
    "<br></br>\n",
    "![](./images/6-1-2-svd.jpg)\n",
    "<br></br>\n",
    "\n",
    "고차원에서 주어진 데이터들을 임의의 주성분 고차원 평면 ($초평면^{hyperplane}$에 투사했을때 투사점들 사이가 서로 최대한 멀어져야 합니다. 즉, 투사점들의 분산이 최대가 되도록 합니다. 또한 고차원 평면으로 투사할대 원래 벡터와 고차원 평면상의 투사된 거리가 최소가 되어야 합니다.\n",
    "\n",
    "주성분 분석을 통해 고차원의 데이터를 더 낮은 차원으로 효과적으로 압축할 수 있습니다. 하지만 앞에서 언급했듯이 실제 데이터의 위치와 고차원 평면에 투사된 점의 거리가 생길 수 밖에 없습니다. 이는 곧 **정보의 손실**을 의미합니다. 특히 주성분은 직선 또는 평면이므로, 이러한 손실은 불가피하게 나타납니다. 이 과정에서 너무 많은 정보가 손실된다면 효율적으로 정보를 학습하거나 복구할 수 없습니다. 따라서 높은 차원에 표현된 정보를 지나치게 낮은 차원으로 축소하여 표현하기는 어렵습니다. 특히 데이터가 비선형적으로 구성될수록 더욱더 힘들어집니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbeb243c",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b07fd9",
   "metadata": {},
   "source": [
    "### 매니폴드 가설"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da0939c",
   "metadata": {},
   "source": [
    "이때 하나의 가설을 통해 차원 축소에 더 효율적으로 접근해볼 수 있습니다. 높은 차원에 존재하는 데이터들의 경우, 실제로는 해당 데이터들을 아우르는 낮은 차원의 $다양체^{manifold}$가 존재한다는 $매니폴드\\ 가설^{manifold\\ hypothesis}$입니다.\n",
    "\n",
    "<br></br>\n",
    "![](./images/6-2-2-manifold.jpg)\n",
    "<br></br>\n",
    "\n",
    "이와 같이 3차원 공간에 분포한 데이터를 아우르는 소용돌이 모양의 구부려진 2차원 매니폴드가 존재할 수도 있습니다. 이런 매니폴드를 찾아 2차원 평면에 데이터 포인트들을 맵핑할 수 있습니다. 그러한 매니폴드를 찾을 수 있다면 앞서 살펴본 주성분 분석처럼 데이터를 고차원 평면에 선형적으로 투사하며 생긴 손실을 최소화할 수 있을것입니다.\n",
    "\n",
    "<br></br>\n",
    "![](./images/6-2-2-manifold2.jpg)\n",
    "<br></br>\n",
    "\n",
    "매니폴드 가설에 따르면 또 하나의 흥미로운 특징이 있습니다. 앞서 그림에서 볼 수 있듯이, 고차원상에서 가까운 거리에 있던 데이터 포인트들일지라도, 매니폴드를 보다 저차원 공간으로맵핑하면 오히려 거리가 멀어질 수 있다는 것입니다. 그리고 저차원의 공간상에서 가까운 점끼리는 실제로도 비슷한 특징을 갖는다는 것입니다. 즉, 저차원의 각 공간의 차원축은 고차원에서 비선형적으로 표현될 것이며, 데이터의 특징을 각각 표현하게 될 것입니다.\n",
    "\n",
    "예를 들어 다음 그림과 같이 MNIST 데이터를 2차원의 숨겨진 저차원^{low-dimensional\\ latent\\ space}에 표현한다고 가정합시다. 빨간색으로 표시된 각 샘플은 2차원 공간에서는 사람이 인지하는 특징과 비슷한 특징을 갖는 위치와 관계에 있겠지만, 원래의 데이터 차원인 784차원의 고차원 공간에서는 전혀 다른 거리와 관계를 지닐 것 입니다.\n",
    "\n",
    "<br></br>\n",
    "![](./images/6-2-2-manifold3.jpg)\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1c3a0e",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab234e75",
   "metadata": {},
   "source": [
    "### 딥러닝이 잘 동작하는 이유"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4683a1ed",
   "metadata": {},
   "source": [
    "아마도 딥러닝이 훌륭한 성능을 내는 이유를 여기서 찾을 수 있을 것입니다. 대부분의 딥러닝이 문제를 풀기 위해 차원 축소를 수행하는 과정은, 데이터가 존재하는 고차원상에서 매니폴드를 찾는 과정입니다. PCA와 같이 다른 선형적인 방식에 비해 딥러닝은 비선형적인 방식으로 차원 축소를 수행하며, 그 과정에서 해당 문제를 가장 잘 해결하기 위한 매니폴드를 자연스럽게 찾아냅니다.\n",
    "\n",
    "<br></br>\n",
    "![](./images/6-2-3-manifold.jpg)\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fa34d2",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe65c31",
   "metadata": {},
   "source": [
    "### Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e610eb",
   "metadata": {},
   "source": [
    "자연어 처리에서 단어를 표현하기 위한 차원 축소를 본격적으로 다루기 전에, **$오토인코더^{autoencoder}$**에 관해 이야기해보겠습니다. 오토인코더는 다음과 같은 구조를 가진 딥러닝 모델입니다.\n",
    "\n",
    "<br></br>\n",
    "![](./images/6-2-4-autoencoder.jpg)\n",
    "<br></br>\n",
    "\n",
    "고차원의 샘플 벡터를 입력으로 받아 매니폴드를 찾고, 저차원으로 축소하는 인코더를 거쳐 병목^{bottleneck} 구간에서의 숨겨진 벡터로 표현합니다. 그리고 디코더는 저차원의 벡터를 받아, 다시 원래 입력 샘플이 존재하던 고차원으로 데이터를 복원하는 작업을 수행합니다. 복원된 데이터는 고차원 상의 매니폴드 위에 위치하게 될 것입니다.\n",
    "\n",
    "이때 고차원의 벡터를 저차원으로 압축한 후 다시 복원하는 과정에서, 오토인코더는 병목의 차원이 매우 낮기때문에 복원에 필요한 정보만 남기고 필요 없는 정보는 버려야합니다. 좁은 병목구간을 통과하기 위해서는 복원에 필요없는 정보부터 버려질 것입니다. 따라서 이 구조의 모델을 훈련할대는 복원된 데이터와 실제 입력 데이터 사이의 차이를 최소화 하도록 손실 함수를 구성합니다.\n",
    "\n",
    "<br></br>\n",
    "![](./images/6-2-4-autoencoder2.jpg)\n",
    "<br></br>\n",
    "\n",
    "하지만 고차원에서 저차원으로 데이터를 표현하면서 손실이 따를 수 있으므로, 훈련이 완료된 모델이지라도 복원된 데이터는 실제 입력과 차이가 있을 수 있습니다.\n",
    "\n",
    "오토인코더를 사용하여 이전 장에서 TF-IDF등을 활용해 계산한 희소 단어 특징 벡터를 입력으로 넣고 같은 출력밧을 갖도록 훈련했을때, 오토인코더의 병목 계층 결괏값을 덴스 단어 임베딩 벡터로 사용할 수 있을 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e83c237",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
